{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T09:11:47.528426Z",
     "start_time": "2025-12-11T09:11:43.970525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "# from unsloth import FastLanguageModel\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "# import bitsandbytes as bnb"
   ],
   "id": "9fd36cea8319b6b9",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-11T09:17:55.917842Z",
     "start_time": "2025-12-11T09:17:55.910107Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "# Конфигурация\n",
    "MAX_SEQ_LENGTH = 1024 # Ограничиваем длину, чтобы сэкономить VRAM T4\n",
    "DTYPE = None # Auto detection\n",
    "LOAD_IN_4BIT = True # [cite: 92] Обязательно для T4\n",
    "\n",
    "class ABSADataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, max_length):\n",
    "        self.data = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Загрузка данных [cite: 48]\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                self.data.append(json.loads(line))\n",
    "\n",
    "        # Шаблон промпта (Alpaca format из твоих данных) [cite: 51]\n",
    "        self.prompt_template = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "\n",
    "        # 1. Формируем полный текст (Prompt + Answer)\n",
    "        full_prompt = self.prompt_template.format(\n",
    "            instruction=item['instruction'],\n",
    "            input=item['input']\n",
    "        )\n",
    "        full_text = full_prompt + item['output'] + self.tokenizer.eos_token\n",
    "\n",
    "        # 2. Токенизация полного текста\n",
    "        encoded = self.tokenizer(\n",
    "            full_text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = encoded[\"input_ids\"][0]\n",
    "        attention_mask = encoded[\"attention_mask\"][0]\n",
    "\n",
    "        # 3. Создаем Labels (Копируем input_ids)\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        # 4. ВАЖНО: Маскируем часть промпта (Instruction + Input), чтобы не считать по ней Loss\n",
    "        # Токенизируем только промпт, чтобы узнать его длину\n",
    "        prompt_tokens = self.tokenizer(\n",
    "            full_prompt,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )[\"input_ids\"][0]\n",
    "\n",
    "        # Длина промпта (без спецтокенов начала, если есть)\n",
    "        prompt_len = prompt_tokens.shape[0]\n",
    "\n",
    "        # Заменяем токены промпта и паддинги на -100 (PyTorch игнорирует -100 при расчете Loss)\n",
    "        labels[:prompt_len] = -100\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "dataset = ABSADataset(\"../notebooks/train_dataset.jsonl\", tokenizer, MAX_SEQ_LENGTH)",
   "id": "4ef0d128d7361c24"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1. Загрузка модели и токенизатора\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/llama-3-8b-bnb-4bit\", # [cite: 79]\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    dtype=DTYPE,\n",
    "    load_in_4bit=LOAD_IN_4BIT,\n",
    ")\n",
    "\n",
    "# Настройка паддинга (Llama-3 не имеет pad token по умолчанию)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 2. Навешиваем LoRA адаптеры [cite: 93]\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=True, # Экономит VRAM\n",
    ")\n",
    "\n",
    "# Переводим в режим обучения\n",
    "model.print_trainable_parameters()"
   ],
   "id": "c9843a079c24e11e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Гиперпараметры\n",
    "BATCH_SIZE = 2      # Маленький физический батч для T4\n",
    "GRAD_ACCUM_STEPS = 4 # Виртуальный батч = 2 * 4 = 8\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 2e-4\n",
    "\n",
    "# Подготовка данных\n",
    "dataset = ABSADataset(\"train_dataset.jsonl\", tokenizer, MAX_SEQ_LENGTH) # [cite: 90]\n",
    "\n",
    "# Разделение на Train/Val (90/10)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Оптимизатор: используем 8-битный AdamW для экономии памяти\n",
    "optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Логгирование\n",
    "history = {'train_loss': [], 'val_loss': []}\n",
    "\n",
    "print(\"Начинаем обучение...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\")\n",
    "\n",
    "    # --- TRAINING PHASE ---\n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        # Перенос на GPU\n",
    "        input_ids = batch['input_ids'].to(\"cuda\")\n",
    "        attention_mask = batch['attention_mask'].to(\"cuda\")\n",
    "        labels = batch['labels'].to(\"cuda\")\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss / GRAD_ACCUM_STEPS # Нормализация лосса\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        if (step + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_train_loss += loss.item() * GRAD_ACCUM_STEPS\n",
    "        progress_bar.set_postfix({'loss': loss.item() * GRAD_ACCUM_STEPS})\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    history['train_loss'].append(avg_train_loss)\n",
    "\n",
    "    # --- VALIDATION PHASE ---\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            input_ids = batch['input_ids'].to(\"cuda\")\n",
    "            attention_mask = batch['attention_mask'].to(\"cuda\")\n",
    "            labels = batch['labels'].to(\"cuda\")\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            total_val_loss += outputs.loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    history['val_loss'].append(avg_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "# Сохранение адаптеров [cite: 95]\n",
    "model.save_pretrained(\"lora_model\")\n",
    "tokenizer.save_pretrained(\"lora_model\")\n",
    "print(\"Обучение завершено и адаптеры сохранены.\")"
   ],
   "id": "ca8d79bf1aba2b6e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_training_history(history):\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, history['train_loss'], 'b-o', label='Training Loss')\n",
    "    plt.plot(epochs, history['val_loss'], 'r-o', label='Validation Loss')\n",
    "\n",
    "    plt.title('Динамика обучения (CrossEntropy Loss)')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history)"
   ],
   "id": "5ef2f7ad99acbc74"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
